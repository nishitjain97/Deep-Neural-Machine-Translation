{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eX-1lnzMymvb"
   },
   "source": [
    "## This file curates a dataset for input to train a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mount the Google drive onto Colab Virtual Environment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "root = '.'\n",
    "# root = '/content/drive/My Drive/English Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwsWQdzgymvf"
   },
   "source": [
    "### 1. Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRVYTn2iymvg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oxF7xoJymvq"
   },
   "source": [
    "### 2. Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_english():\n",
    "    \"\"\"\n",
    "        Call this function to merge cleaned english texts for word2vec model.\n",
    "    \"\"\"\n",
    "    # Open dataset\n",
    "    lines = pd.read_pickle(os.path.join(root, 'hin-eng_cleaned.parallel'))\n",
    "    \n",
    "    # Open output_file\n",
    "    f_out = open(os.path.join(root, 'mono.en'), 'w')\n",
    "    \n",
    "    # Iterate over english sentences\n",
    "    for line in lines.Eng:\n",
    "        # Remove the next line character\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Write to file while appending space.\n",
    "        f_out.write(line + ' ')\n",
    "        \n",
    "        \n",
    "    # Open second dataset\n",
    "    lines = pd.read_pickle(os.path.join(root, 'mar-eng_cleaned.parallel'))\n",
    "    \n",
    "    # Iterate over english sentences\n",
    "    for line in lines.Eng:\n",
    "        # Remove new-line character\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Write to file\n",
    "        f_out.write(line + ' ')\n",
    "        \n",
    "    # Close file\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hindi():\n",
    "    \"\"\"\n",
    "        Call this function to merge cleaned Hindi texts for word2vec model.\n",
    "    \"\"\"\n",
    "    # Open dataset\n",
    "    lines = pd.read_pickle(os.path.join(root, 'hin-eng_cleaned.parallel'))\n",
    "    \n",
    "    # Open output file\n",
    "    f_out = open(os.path.join(root, 'mono.hi'), 'w')\n",
    "    \n",
    "    for line in lines.Hin:\n",
    "        # Remove new-line character\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Write to file\n",
    "        f_out.write(line + ' ')\n",
    "        \n",
    "    # Close file\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Marathi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_marathi():\n",
    "    \"\"\"\n",
    "        Call this function to merge cleaned Marathi texts for word2vec model.\n",
    "    \"\"\"\n",
    "    # Read dataset\n",
    "    lines = pd.read_pickle(os.path.join(root, 'mar-eng_cleaned.parallel'))\n",
    "    \n",
    "    # Open output file\n",
    "    f_out = open(os.path.join(root, 'mono.ma'), 'w')\n",
    "    \n",
    "    for line in lines.Mar:\n",
    "        # Remove new-line character\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Write to file\n",
    "        f_out.write(line + ' ')\n",
    "        \n",
    "    # Close file\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMF7YWlqS-gn"
   },
   "outputs": [],
   "source": [
    "# merge_english()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_hindi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_marathi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_Gather_Dataset.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
