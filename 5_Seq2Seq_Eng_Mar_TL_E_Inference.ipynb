{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mount Google drive on Google Colab environment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# root = '/content/drive/My Drive/English Dataset'\n",
    "root = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum lengths of source and target\n",
    "max_length_src = 34\n",
    "max_length_tar = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary and embeddings\n",
    "with open(os.path.join(root, 'data/embeddings/embeddings.en'), 'rb') as f:\n",
    "    english_summary = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(root, 'data/embeddings/embeddings.ma'), 'rb') as f:\n",
    "    marathi_summary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end tokens to dictionary\n",
    "for word in ['<START>', '<END>']:\n",
    "    l = len(marathi_summary['dictionary'].keys())\n",
    "    marathi_summary['dictionary'][word] = l\n",
    "    marathi_summary['reverse_dictionary'][l] = word\n",
    "    marathi_summary['embeddings'] = np.vstack((marathi_summary['embeddings'], np.zeros((1, marathi_summary['embeddings'].shape[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English vocabulary\n",
    "all_eng_words = set(list(english_summary['dictionary'].keys()))\n",
    "        \n",
    "# Marathi vocabulary\n",
    "all_mar_words = set(list(marathi_summary['dictionary'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8731, 12698)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_mar_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dictionary = english_summary['dictionary']\n",
    "target_dictionary = marathi_summary['dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_reverse_dictionary = english_summary['reverse_dictionary']\n",
    "target_reverse_dictionary = marathi_summary['reverse_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(X):\n",
    "    \"\"\"\n",
    "        X = batch of inputs\n",
    "    \"\"\"\n",
    "    # Get the batch_size\n",
    "    batch_size = len(X)\n",
    "    \n",
    "    # Create a numpy array of zeros to hold input\n",
    "    encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(X):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            if word not in source_dictionary.keys():\n",
    "                word = 'UNK'\n",
    "            encoder_input_data[i, t] = source_dictionary[word]\n",
    "            \n",
    "    return encoder_input_data\n",
    "\n",
    "def encode_target(y):\n",
    "    \"\"\"\n",
    "        y = batch of outputs\n",
    "    \"\"\"\n",
    "    # Get the batch_size\n",
    "    batch_size = len(y)\n",
    "    \n",
    "    # Create numpy arrays of zeros to hold encoded targets\n",
    "    decoder_input_data = np.zeros((batch_size, max_length_tar), dtype='float32')\n",
    "    decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
    "    \n",
    "    for i, target_text in enumerate(y):\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "            if t < len(target_text.split()) - 1:\n",
    "                decoder_input_data[i, t] = target_dictionary[word]\n",
    "                \n",
    "            if t > 0:\n",
    "                decoder_target_data[i, t-1, target_dictionary[word]] = 1.0\n",
    "                \n",
    "    return decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, y, batch_size=128):\n",
    "    \"\"\"\n",
    "        X = Source dataset\n",
    "        y = Target dataset\n",
    "        batch_size = Size of each batch\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = encode_input(X[j:j+batch_size])\n",
    "            decoder_input_data, decoder_target_data = encode_target(y[j:j+batch_size])\n",
    "            \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "encoder_inputs = Input(shape=(None, ), name='Encoder_Inputs')\n",
    "\n",
    "# Embedding Lookup\n",
    "encoder_embedding_layer = Embedding(num_encoder_tokens, latent_dim, mask_zero=True, \n",
    "                                    name='English_Embedding_Layer')\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "# LSTM\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='Encoder_LSTM')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
    "\n",
    "# Keeping only the states and discarding encoder outputs\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "decoder_inputs = Input(shape=(None, ), name='Decoder_Inputs')\n",
    "\n",
    "# Embedding\n",
    "decoder_embedding_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True, \n",
    "                                    name='Marathi_Embedding_Layer')\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
    "\n",
    "# Dense output layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Decoder_Dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with these layers\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained weights from Eng-Hin model\n",
    "model.load_weights(os.path.join(root, 'models/best_model_en_ma_tl_e.hdf5'), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder model that uses trained weights from the original model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model to create a thought vector from the input\n",
    "inference_encoder = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each time step, the decoder states from previous timestep would act as inputs\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ), name='Inference_Decoder_Output')\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ), name='Inference_Decoder_Memory')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Embedding\n",
    "decoder_embeddings_inference = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# LSTM\n",
    "decoder_outputs_inference, state_h_inference, state_c_inference = decoder_lstm(decoder_embeddings_inference, \n",
    "                                                                               initial_state=decoder_states_inputs)\n",
    "decoder_states_inference = [state_h_inference, state_c_inference]\n",
    "\n",
    "# Dense\n",
    "decoder_outputs_inference = decoder_dense(decoder_outputs_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "inference_decoder = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs_inference] + decoder_states_inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sequence):\n",
    "    # Get thought vector by encoding the input sequence\n",
    "    states_value = inference_encoder.predict(input_sequence)\n",
    "    \n",
    "    # Generate target sequence initialized with <START> character\n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = target_dictionary['<START>']\n",
    "    \n",
    "    # To stop the recurrent loop\n",
    "    stop_condition = False\n",
    "    \n",
    "    # Final sentence\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Get next prediction\n",
    "        output_tokens, h, c = inference_decoder.predict([target_sequence] + states_value)\n",
    "        \n",
    "        # Get the token with max probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = target_reverse_dictionary[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        # Test for exit condition\n",
    "        if (sampled_word == '<END>') or (len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "            \n",
    "        # Update the target sequence with current prediction\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_pickle(os.path.join(root, 'data', 'mar-eng_cleaned.parallel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33725, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = lines.Eng, lines.Mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = list()\n",
    "references = list()\n",
    "\n",
    "for input_sentence, reference in zip(X, y):\n",
    "    input_sequence = encode_input([input_sentence])\n",
    "    decoded_sentence = decode_sequence(input_sequence).split()[:-1]\n",
    "    candidates.append(decoded_sentence)\n",
    "    references.append([reference.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = corpus_bleu(references, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.432091268942226"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
