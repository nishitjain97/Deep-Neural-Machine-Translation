{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Seq2Seq_Eng_Mar_TL_E_Inference.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxzf3GOVP3i6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5f145b61-ee84-45b5-967c-073a0543a655"
      },
      "source": [
        "# To mount Google drive on Google Colab environment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/My Drive/English Dataset'\n",
        "# root = '.'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubyk8uuyP3jE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b78607d0-a05a-4374-c601-5abec7498d25"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from string import digits"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCP99-0kP3jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define maximum lengths of source and target\n",
        "max_length_src = 34\n",
        "max_length_tar = 37"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsAx3r7-P3jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get vocabulary and embeddings\n",
        "with open(os.path.join(root, 'embeddings.en'), 'rb') as f:\n",
        "    english_summary = pickle.load(f)\n",
        "    \n",
        "with open(os.path.join(root, 'embeddings.ma'), 'rb') as f:\n",
        "    marathi_summary = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_RxrlaSP3jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add start and end tokens to dictionary\n",
        "for word in ['<START>', '<END>']:\n",
        "    l = len(marathi_summary['dictionary'].keys())\n",
        "    marathi_summary['dictionary'][word] = l\n",
        "    marathi_summary['reverse_dictionary'][l] = word\n",
        "    marathi_summary['embeddings'] = np.vstack((marathi_summary['embeddings'], np.zeros((1, marathi_summary['embeddings'].shape[1]))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1xGlqCP3jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English vocabulary\n",
        "all_eng_words = set(list(english_summary['dictionary'].keys()))\n",
        "        \n",
        "# Marathi vocabulary\n",
        "all_mar_words = set(list(marathi_summary['dictionary'].keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SW_FrpBP3jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71c1d8c5-ee59-45ed-9eae-b185e83092c4"
      },
      "source": [
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_mar_words)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8731, 12698)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ayp-yubP3jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_dictionary = english_summary['dictionary']\n",
        "target_dictionary = marathi_summary['dictionary']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm4Qu37BP3j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_reverse_dictionary = english_summary['reverse_dictionary']\n",
        "target_reverse_dictionary = marathi_summary['reverse_dictionary']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmGNP6GYP3j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_input(X):\n",
        "    \"\"\"\n",
        "        X = batch of inputs\n",
        "    \"\"\"\n",
        "    # Get the batch_size\n",
        "    batch_size = len(X)\n",
        "    \n",
        "    # Create a numpy array of zeros to hold input\n",
        "    encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')\n",
        "    \n",
        "    for i, input_text in enumerate(X):\n",
        "        for t, word in enumerate(input_text.split()):\n",
        "            if word not in source_dictionary.keys():\n",
        "                word = 'UNK'\n",
        "            encoder_input_data[i, t] = source_dictionary[word]\n",
        "            \n",
        "    return encoder_input_data\n",
        "\n",
        "def encode_target(y):\n",
        "    \"\"\"\n",
        "        y = batch of outputs\n",
        "    \"\"\"\n",
        "    # Get the batch_size\n",
        "    batch_size = len(y)\n",
        "    \n",
        "    # Create numpy arrays of zeros to hold encoded targets\n",
        "    decoder_input_data = np.zeros((batch_size, max_length_tar), dtype='float32')\n",
        "    decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
        "    \n",
        "    for i, target_text in enumerate(y):\n",
        "        for t, word in enumerate(target_text.split()):\n",
        "            if t < len(target_text.split()) - 1:\n",
        "                decoder_input_data[i, t] = target_dictionary[word]\n",
        "                \n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t-1, target_dictionary[word]] = 1.0\n",
        "                \n",
        "    return decoder_input_data, decoder_target_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9SylykpP3kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X, y, batch_size=128):\n",
        "    \"\"\"\n",
        "        X = Source dataset\n",
        "        y = Target dataset\n",
        "        batch_size = Size of each batch\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = encode_input(X[j:j+batch_size])\n",
        "            decoder_input_data, decoder_target_data = encode_target(y[j:j+batch_size])\n",
        "            \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OMAtPduP3kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 128\n",
        "batch_size = 128\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtq7-5anP3kH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b5f08f17-c51f-4226-966a-54d0124ba391"
      },
      "source": [
        "# Inputs\n",
        "encoder_inputs = Input(shape=(None, ), name='Encoder_Inputs')\n",
        "\n",
        "# Embedding Lookup\n",
        "encoder_embedding_layer = Embedding(num_encoder_tokens, latent_dim, mask_zero=True, \n",
        "                                    name='English_Embedding_Layer')\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "\n",
        "# LSTM\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, name='Encoder_LSTM')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "# Keeping only the states and discarding encoder outputs\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg90nrRQP3kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inputs\n",
        "decoder_inputs = Input(shape=(None, ), name='Decoder_Inputs')\n",
        "\n",
        "# Embedding\n",
        "decoder_embedding_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True, \n",
        "                                    name='Marathi_Embedding_Layer')\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "# LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
        "\n",
        "# Dense output layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Decoder_Dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBi84QCLP3kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a model with these layers\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XELaxazLP3kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained weights from Eng-Hin model\n",
        "model.load_weights(os.path.join(root, 'best_model_en_ma_tl_e.hdf5'), by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaNeOLAeP3kX",
        "colab_type": "text"
      },
      "source": [
        "### Encoder-decoder model that uses trained weights from the original model to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmouWLWuP3kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder model to create a thought vector from the input\n",
        "inference_encoder = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyEdmXrwP3kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each time step, the decoder states from previous timestep would act as inputs\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ), name='Inference_Decoder_Output')\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ), name='Inference_Decoder_Memory')\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Embedding\n",
        "decoder_embeddings_inference = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "# LSTM\n",
        "decoder_outputs_inference, state_h_inference, state_c_inference = decoder_lstm(decoder_embeddings_inference, \n",
        "                                                                               initial_state=decoder_states_inputs)\n",
        "decoder_states_inference = [state_h_inference, state_c_inference]\n",
        "\n",
        "# Dense\n",
        "decoder_outputs_inference = decoder_dense(decoder_outputs_inference)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_x2BlrP3ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder model\n",
        "inference_decoder = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs_inference] + decoder_states_inference\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmQVNt7gP3kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_sequence):\n",
        "    # Get thought vector by encoding the input sequence\n",
        "    states_value = inference_encoder.predict(input_sequence)\n",
        "    \n",
        "    # Generate target sequence initialized with <START> character\n",
        "    target_sequence = np.zeros((1, 1))\n",
        "    target_sequence[0, 0] = target_dictionary['<START>']\n",
        "    \n",
        "    # To stop the recurrent loop\n",
        "    stop_condition = False\n",
        "    \n",
        "    # Final sentence\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    while not stop_condition:\n",
        "        # Get next prediction\n",
        "        output_tokens, h, c = inference_decoder.predict([target_sequence] + states_value)\n",
        "        \n",
        "        # Get the token with max probability\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_reverse_dictionary[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "        \n",
        "        # Test for exit condition\n",
        "        if (sampled_word == '<END>') or (len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # Update the target sequence with current prediction\n",
        "        target_sequence = np.zeros((1, 1))\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "        \n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYSXwN-tQw7W",
        "colab_type": "text"
      },
      "source": [
        "### Inference Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q78GBj3AQs5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18a41e96-c2e2-4619-f578-143508c1ab2e"
      },
      "source": [
        "input_sentence = input()\n",
        "input_sequence = encode_input([input_sentence])\n",
        "decoded_sentence = ' '.join(decode_sequence(input_sequence).split()[:-1])\n",
        "print(decoded_sentence)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i am the best\n",
            "मी सर्वात चांगलं आहे\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KdrRQEbP3kz",
        "colab_type": "text"
      },
      "source": [
        "### BLEU evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUkWnRwwP3k1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lines = pd.read_pickle(os.path.join(root, 'data', 'mar-eng_cleaned.parallel'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgFqE4cfP3k4",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c77b2f1-313c-416f-c97d-33899e1c3079"
      },
      "source": [
        "# lines.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33725, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w2BrVkKP3k_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X, y = lines.Eng, lines.Mar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiRk_GskP3lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# candidates = list()\n",
        "# references = list()\n",
        "\n",
        "# for input_sentence, reference in zip(X, y):\n",
        "#     input_sequence = encode_input([input_sentence])\n",
        "#     decoded_sentence = decode_sequence(input_sequence).split()[:-1]\n",
        "#     candidates.append(decoded_sentence)\n",
        "#     references.append([reference.split()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p-tjXv8P3lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# score = corpus_bleu(references, candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWpb_MpvP3lU",
        "colab_type": "code",
        "colab": {},
        "outputId": "c60de0ce-f020-44b1-8e3f-c06f58de18f9"
      },
      "source": [
        "# score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.432091268942226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQPF6cJgP3lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}